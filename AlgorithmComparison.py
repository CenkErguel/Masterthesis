# -*- coding: utf-8 -*-
"""AlgorithmComparison.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QU9qG0BvgdrPm2FI5kploKn4w5x6ScgS

### **2-CLASS CLASSIFICATION**
"""

#Importing Libraries
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import LinearSVC
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
import matplotlib.pyplot as plt
import nltk
import numpy as np
import os
import pandas as pd
import re
import tensorflow as tf
import xgboost

df = pd.read_csv('/content/Dataset.csv', sep = ';')

# All 3 star ratings are deleted
df_binary = df[df['Rating'] != 3]

# Set index starting at 0
df_binary.index = np.arange(0, len(df_binary))

# Train- and Testdata of DataFrame
df_train_binary = df_binary.iloc[:355562, 8:]
df_test_binary = df_binary.iloc[355562:, 8:]

# Create Rating Column as own Series
train_rating_binary = df_binary.iloc[:355562, 9:]
test_rating_binary = df_binary.iloc[355562:, 9:]

# Create Comment Column as own Series
train_comment_binary = df_binary.iloc[:355562, 8]
test_comment_binary = df_binary.iloc[355562:, 8]

i = 0
train_rating_binary_2 = []
for i in train_rating_binary['Rating']:
    if i == 1 or i == 2:
        i = 0
    elif i == 4 or i == 5:
        i = 1
    train_rating_binary_2.append(i)

j = 0
test_rating_binary_2 = []
for j in test_rating_binary['Rating']:
    if j == 1 or j == 2:
        j = 0
    elif j == 4 or j == 5:
        j = 1
    test_rating_binary_2.append(j)

nltk.download('stopwords')

# Use Regex for data cleaning for train_comment
for item in range(len(train_comment_binary)):
    # Delete specific characters
    train_comment_binary[item] = re.sub('[-*:;()"=+!]', ' ', train_comment_binary[item])
    # Delete whitespaces at the beginning and at the end of the sentence
    train_comment_binary[item] = re.sub('^\s+|\s+$', '', train_comment_binary[item])
    # Delete double dots
    train_comment_binary[item] = re.sub('\.\.+', ' ', train_comment_binary[item])
    # Delete double whitespaces
    train_comment_binary[item] = re.sub(' +', ' ', train_comment_binary[item])

# Create Train-Dataset where stop words are removed
german_stop_words = stopwords.words('german') 
def remove_stop_words(corpus):
     removed_stop_words = []
     for review in corpus:
         removed_stop_words.append(
             ' '.join([word for word in review.split()
                       if word not in german_stop_words])
         )
     return removed_stop_words

train_comment_sw = remove_stop_words(train_comment_binary)

# Stemming
def get_stemmed_text(corpus):
    stemmer = SnowballStemmer("german")
    return [' '.join([stemmer.stem(word) for word in review.split()]) for review in corpus]

stemmed_reviews = get_stemmed_text(train_comment_sw)

# Convert List into Series
stemmed_reviews = pd.Series(stemmed_reviews)

cv = CountVectorizer(binary=False, ngram_range=(1, 3))
cv.fit(train_comment_binary.values.astype('U'))
X = cv.transform(train_comment_binary.values.astype('U'))
X_test = cv.transform(test_comment_binary.values.astype('U'))

X_train, X_val, y_train, y_val = train_test_split(X, 
                                                  train_rating_binary_2,
                                                  train_size = 0.75
                                                  )

cv = CountVectorizer(binary=True, ngram_range=(1, 3))
cv.fit(stemmed_reviews.values.astype('U'))
X = cv.transform(stemmed_reviews.values.astype('U'))
X_test = cv.transform(test_comment_binary.values.astype('U'))

X_train, X_val, y_train, y_val = train_test_split(X,
                                                  train_rating_binary_2,
                                                  train_size = 0.75
                                                  )

tf = TfidfVectorizer(binary=False, ngram_range=(1, 3))
tf.fit(train_comment_binary.values.astype('U'))
X = tf.transform(train_comment_binary.values.astype('U'))
X_test = tf.transform(test_comment_binary.values.astype('U'))

X_train, X_val, y_train, y_val = train_test_split(X,
                                                  train_rating_binary_2,
                                                  train_size = 0.75
                                                  )

"""**Logistic Regression**"""

lr = LogisticRegression(C=1, max_iter=500)
lr.fit(X_train, y_train)
print(classification_report(y_val, lr.predict(X_val), digits=4))

"""**SVM**"""

svm = LinearSVC(C=0.1)
svm.fit(X_train, y_train)
print(classification_report(y_val, svm.predict(X_val), digits=4))

"""**Multinomial Naive Bayes**"""

nb = MultinomialNB()
nb.fit(X_train, y_train)
print(classification_report(y_val, nb.predict(X_val), digits=4))

"""**Decision Tree**"""

dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
print(classification_report(y_val, dt.predict(X_val), digits=4))

"""**XG Boost**"""

xgb = XGBClassifier(learning_rate=0.9)
xgb.fit(X_train, y_train)
print(classification_report(y_val, xgb.predict(X_val), digits=4))

"""**KNN**"""

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)
print(classification_report(y_val, knn.predict(X_val), digits=4))

"""**NN**"""

classifier = MLPClassifier(hidden_layer_sizes=(10,10,10), 
                           max_iter=500,activation = 'relu',
                           solver='adam',random_state=1)
classifier.fit(X_train, y_train)
print(classification_report(y_val, classifier.predict(X_val), digits=4))

"""### **3-CLASS CLASSIFICATION**"""

#Importing Libraries
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import LinearSVC
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
import matplotlib.pyplot as plt
import nltk
import numpy as np
import os
import pandas as pd
import re
import tensorflow as tf
import xgboost

df = pd.read_csv('/content/Dataset.csv', sep = ';')

# Set index starting at 0
#df_3_class.index = np.arange(0, len(df))

# Get last index number
last_index = len(df)-1

# Index for Train-, Test-Split
index = int(last_index * 0.8)

# Train- and Testdata of DataFrame
df_train_3_class = df.iloc[:index, 8:]
df_test_3_class = df.iloc[index:, 8:]

# Create Rating Column as own Series
train_rating_3_class = df.iloc[:index, 9:]
test_rating_3_class = df.iloc[index:, 9:]

# Create Comment Column as own Series
train_comment_3_class = df.iloc[:index, 8]
test_comment_3_class = df.iloc[index:, 8]

i = 0
train_rating_3_class_new = []
for i in train_rating_3_class['Rating']:
    if i == 1 or i == 2:
        i = 0
    elif i == 4 or i == 5:
        i = 1
    train_rating_3_class_new.append(i)

j = 0
test_rating_3_class_new = []
for j in test_rating_3_class['Rating']:
    if j == 1 or j == 2:
        j = 0
    elif j == 4 or j == 5:
        j = 1
    test_rating_3_class_new.append(j)

# Use Regex for data cleaning for train_comment
for item in range(len(train_comment_3_class)):
    # Delete specific characters
    train_comment_3_class[item] = re.sub('[-*:;()"=+!]', ' ', train_comment_3_class[item])
    # Delete whitespaces at the beginning and at the end of the sentence
    train_comment_3_class[item] = re.sub('^\s+|\s+$', '', train_comment_3_class[item])
    # Delete double dots
    train_comment_3_class[item] = re.sub('\.\.+', ' ', train_comment_3_class[item])
    # Delete double whitespaces
    train_comment_3_class[item] = re.sub(' +', ' ', train_comment_3_class[item])

nltk.download('stopwords')

# Create Train-Dataset where stop words are removed
german_stop_words = stopwords.words('german') 
def remove_stop_words(corpus):
     removed_stop_words = []
     for review in corpus:
         removed_stop_words.append(
             ' '.join([word for word in review.split()
                       if word not in german_stop_words])
         )
     return removed_stop_words

train_comment_3_class_sw = remove_stop_words(train_comment_3_class)

# Stemming
def get_stemmed_text(corpus):
    stemmer = SnowballStemmer("german")
    return [' '.join([stemmer.stem(word) for word in review.split()]) for review in corpus]

stemmed_reviews = get_stemmed_text(train_comment_3_class_sw)

# Convert List into Series
stemmed_reviews = pd.Series(stemmed_reviews)

cv = CountVectorizer(binary=False, ngram_range=(1, 3))
cv.fit(stemmed_reviews.values.astype('U'))
X = cv.transform(stemmed_reviews.values.astype('U'))
X_test = cv.transform(test_comment_3_class.values.astype('U'))

X_train, X_val, y_train, y_val = train_test_split(X, 
                                                  train_rating_3_class_new,
                                                  train_size = 0.75)

cv = CountVectorizer(binary=False, ngram_range=(1, 3))
cv.fit(train_comment_3_class.values.astype('U'))
X = cv.transform(train_comment_3_class.values.astype('U'))
X_test = cv.transform(test_comment_3_class.values.astype('U'))

X_train, X_val, y_train, y_val = train_test_split(X, 
                                                  train_rating_3_class_new, 
                                                  train_size = 0.75)

tf = TfidfVectorizer(binary=False, ngram_range=(1, 3))
tf.fit(stemmed_reviews.values.astype('U'))
X = tf.transform(stemmed_reviews.values.astype('U'))
X_test = tf.transform(test_comment_3_class.values.astype('U'))

X_train, X_val, y_train, y_val = train_test_split(X,
                                                  train_rating_3_class_new,
                                                  train_size = 0.75)

tf = TfidfVectorizer(binary=False, ngram_range=(1, 3))
tf.fit(train_comment_3_class.values.astype('U'))
X = tf.transform(train_comment_3_class.values.astype('U'))
X_test = tf.transform(test_comment_3_class.values.astype('U'))

X_train, X_val, y_train, y_val = train_test_split(X,
                                                  train_rating_3_class_new,
                                                  train_size = 0.75)

"""**Naive Bayes**"""

nb = MultinomialNB()
nb.fit(X_train, y_train)
print(classification_report(y_val, nb.predict(X_val), digits=4))

"""**Logistic Regression**"""

lr = LogisticRegression(C=1, max_iter=500)
lr.fit(X_train, y_train)
print(classification_report(y_val, lr.predict(X_val), digits=4))

"""**SVM**"""

svm = LinearSVC(C=0.1)
svm.fit(X_train, y_train)
print(classification_report(y_val, svm.predict(X_val), digits=4))

"""**Decision Tree**"""

dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
print(classification_report(y_val, dt.predict(X_val), digits=4))

"""**XGBoost**"""

xgb = XGBClassifier(learning_rate=0.9)
xgb.fit(X_train, y_train)
print(classification_report(y_val, xgb.predict(X_val), digits=4))

"""**KNN**"""

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)
print(classification_report(y_val, knn.predict(X_val), digits=4))

"""**NN**"""

classifier = MLPClassifier(hidden_layer_sizes=(10,10,10), 
                           max_iter=500,activation = 'relu',
                           solver='adam',
                           random_state=1)
classifier.fit(X_train, y_train)
print(classification_report(y_val, classifier.predict(X_val), digits=4))

"""### **4-CLASS CLASSIFICATION**"""

#Importing Libraries
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import LinearSVC
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
import matplotlib.pyplot as plt
import nltk
import numpy as np
import os
import pandas as pd
import re
import tensorflow as tf
import xgboost

df = pd.read_csv('/content/Dataset.csv', sep = ';')

# All 3 star ratings are deleted
df_four = df[df['Rating'] != 3]

# Set index starting at 0
df_four.index = np.arange(0, len(df_four))

# Get last index number
last_index = len(df)-1

# Index for Train-, Test-Split
index = int(last_index * 0.8)

# Train- and Testdata of DataFrame
df_train_4_class = df_four.iloc[:index, 8:]
df_test_4_class = df_four.iloc[index:, 8:]

# Create Rating Column as own Series
train_rating_4_class = df_four.iloc[:index, 9:]
test_rating_4_class = df_four.iloc[index:, 9:]

# Create Comment Column as own Series
train_comment_4_class = df_four.iloc[:index, 8]
test_comment_4_class = df_four.iloc[index:, 8]

# Use Regex for data cleaning for train_comment
for item in range(len(train_comment_4_class)):
    # Delete specific characters
    train_comment_4_class[item] = re.sub('[-*:;()"=+!]', ' ', train_comment_4_class[item])
    # Delete whitespaces at the beginning and at the end of the sentence
    train_comment_4_class[item] = re.sub('^\s+|\s+$', '', train_comment_4_class[item])
    # Delete double dots
    train_comment_4_class[item] = re.sub('\.\.+', ' ', train_comment_4_class[item])
    # Delete double whitespaces
    train_comment_4_class[item] = re.sub(' +', ' ', train_comment_4_class[item])

nltk.download('stopwords')

# Create Train-Dataset where stop words are removed
german_stop_words = stopwords.words('german') 
def remove_stop_words(corpus):
     removed_stop_words = []
     for review in corpus:
         removed_stop_words.append(
             ' '.join([word for word in review.split()
                       if word not in german_stop_words])
         )
     return removed_stop_words

train_comment_4_class_sw = remove_stop_words(train_comment_4_class)

# Stemming
def get_stemmed_text(corpus):
    stemmer = SnowballStemmer("german")
    return [' '.join([stemmer.stem(word) for word in review.split()]) for review in corpus]

stemmed_reviews = get_stemmed_text(train_comment_4_class_sw)

# Convert List into Series
stemmed_reviews = pd.Series(stemmed_reviews)

cv = CountVectorizer(binary=False, ngram_range=(1, 3))
cv.fit(stemmed_reviews.values.astype('U'))
X = cv.transform(stemmed_reviews.values.astype('U'))
X_test = cv.transform(test_comment_4_class.values.astype('U'))

X_train, X_val, y_train, y_val = train_test_split(X, 
                                                  train_rating_4_class,
                                                  train_size = 0.75)

cv = CountVectorizer(binary=False, ngram_range=(1, 3))
cv.fit(train_comment_4_class.values.astype('U'))
X = cv.transform(train_comment_4_class.values.astype('U'))
X_test = cv.transform(test_comment_4_class.values.astype('U'))

X_train, X_val, y_train, y_val = train_test_split(X, 
                                                  train_rating_4_class, 
                                                  train_size = 0.75)

tf = TfidfVectorizer(binary=False, ngram_range=(1, 3))
tf.fit(stemmed_reviews.values.astype('U'))
X = tf.transform(stemmed_reviews.values.astype('U'))
X_test = tf.transform(test_comment_4_class.values.astype('U'))

X_train, X_val, y_train, y_val = train_test_split(X,
                                                  train_rating_4_class,
                                                  train_size = 0.75)

tf = TfidfVectorizer(binary=False, ngram_range=(1, 3))
tf.fit(train_comment_4_class.values.astype('U'))
X = tf.transform(train_comment_4_class.values.astype('U'))
X_test = tf.transform(test_comment_4_class.values.astype('U'))

X_train, X_val, y_train, y_val = train_test_split(X,
                                                  train_rating_4_class,
                                                  train_size = 0.75)

"""**Naive Bayes**"""

nb = MultinomialNB()
nb.fit(X_train, y_train)
print(classification_report(y_val, nb.predict(X_val), digits=4))

"""**Logistic Regression**"""

lr = LogisticRegression(C=1, max_iter=500)
lr.fit(X_train, y_train)
print(classification_report(y_val, lr.predict(X_val), digits=4))

"""**SVM**"""

svm = LinearSVC(C=0.1)
svm.fit(X_train, y_train)
print(classification_report(y_val, svm.predict(X_val), digits=4))

"""**Decision Tree**"""

dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
print(classification_report(y_val, dt.predict(X_val), digits=4))

"""**XGBoost**"""

xgb = XGBClassifier(learning_rate=0.9)
xgb.fit(X_train, y_train)
print(classification_report(y_val, xgb.predict(X_val), digits=4))

"""**KNN**"""

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)
print(classification_report(y_val, knn.predict(X_val), digits=4))

"""**NN**"""

classifier = MLPClassifier(hidden_layer_sizes=(10,10,10), 
                           max_iter=500,activation = 'relu',
                           solver='adam',
                           random_state=1)
classifier.fit(X_train, y_train)
print(classification_report(y_val, classifier.predict(X_val), digits=4))

"""### **5-CLASS CLASSIFICATION**"""

#Importing Libraries
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import LinearSVC
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
import matplotlib.pyplot as plt
import nltk
import numpy as np
import os
import pandas as pd
import re
import tensorflow as tf
import xgboost

df = pd.read_csv('/content/Dataset.csv', sep = ';')

# Get last index number
last_index = len(df)-1

# Index for Train-, Test-Split
index = int(last_index * 0.8)

# Train- and Testdata of DataFrame
df_train_5_class = df.iloc[:index, 8:]
df_test_5_class = df.iloc[index:, 8:]

# Create Rating Column as own Series
train_rating_5_class = df.iloc[:index, 9:]
test_rating_5_class = df.iloc[index:, 9:]

# Create Comment Column as own Series
train_comment_5_class = df.iloc[:index, 8]
test_comment_5_class = df.iloc[index:, 8]

# Use Regex for data cleaning for train_comment
for item in range(len(train_comment_5_class)):
    # Delete specific characters
    train_comment_5_class[item] = re.sub('[-*:;()"=+!]', ' ', train_comment_5_class[item])
    # Delete whitespaces at the beginning and at the end of the sentence
    train_comment_5_class[item] = re.sub('^\s+|\s+$', '', train_comment_5_class[item])
    # Delete double dots
    train_comment_5_class[item] = re.sub('\.\.+', ' ', train_comment_5_class[item])
    # Delete double whitespaces
    train_comment_5_class[item] = re.sub(' +', ' ', train_comment_5_class[item])

nltk.download('stopwords')

# Create Train-Dataset where stop words are removed
german_stop_words = stopwords.words('german') 
def remove_stop_words(corpus):
     removed_stop_words = []
     for review in corpus:
         removed_stop_words.append(
             ' '.join([word for word in review.split()
                       if word not in german_stop_words])
         )
     return removed_stop_words

train_comment_5_class_sw = remove_stop_words(train_comment_5_class)

# Stemming
def get_stemmed_text(corpus):
    stemmer = SnowballStemmer("german")
    return [' '.join([stemmer.stem(word) for word in review.split()]) for review in corpus]

stemmed_reviews = get_stemmed_text(train_comment_5_class_sw)

# Convert List into Series
stemmed_reviews = pd.Series(stemmed_reviews)

cv = CountVectorizer(binary=False, ngram_range=(1, 3))
cv.fit(stemmed_reviews.values.astype('U'))
X = cv.transform(stemmed_reviews.values.astype('U'))
X_test = cv.transform(test_comment_5_class.values.astype('U'))

X_train, X_val, y_train, y_val = train_test_split(X, 
                                                  train_rating_5_class,
                                                  train_size = 0.75)

cv = CountVectorizer(binary=False, ngram_range=(1, 3))
cv.fit(train_comment_5_class.values.astype('U'))
X = cv.transform(train_comment_5_class.values.astype('U'))
X_test = cv.transform(test_comment_5_class.values.astype('U'))

X_train, X_val, y_train, y_val = train_test_split(X, 
                                                  train_rating_5_class, 
                                                  train_size = 0.75)

tf = TfidfVectorizer(binary=False, ngram_range=(1, 3))
tf.fit(stemmed_reviews.values.astype('U'))
X = tf.transform(stemmed_reviews.values.astype('U'))
X_test = tf.transform(test_comment_5_class.values.astype('U'))

X_train, X_val, y_train, y_val = train_test_split(X,
                                                  train_rating_5_class,
                                                  train_size = 0.75)

tf = TfidfVectorizer(binary=False, ngram_range=(1, 3))
tf.fit(train_comment_5_class.values.astype('U'))
X = tf.transform(train_comment_5_class.values.astype('U'))
X_test = tf.transform(test_comment_5_class.values.astype('U'))

X_train, X_val, y_train, y_val = train_test_split(X,
                                                  train_rating_5_class,
                                                  train_size = 0.75)

"""**Naive Bayes**"""

nb = MultinomialNB()
nb.fit(X_train, y_train)
print(classification_report(y_val, nb.predict(X_val), digits=4))

"""**Logistic Regression**"""

lr = LogisticRegression(C=1, max_iter=500)
lr.fit(X_train, y_train)
print(classification_report(y_val, lr.predict(X_val), digits=4))

"""**SVM**"""

svm = LinearSVC(C=0.1)
svm.fit(X_train, y_train)
print(classification_report(y_val, svm.predict(X_val), digits=4))

"""**Decision Tree**"""

dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
print(classification_report(y_val, dt.predict(X_val), digits=4))

"""**XGBoost**"""

xgb = XGBClassifier(learning_rate=0.9)
xgb.fit(X_train, y_train)
print(classification_report(y_val, xgb.predict(X_val), digits=4))

"""**KNN**"""

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)
print(classification_report(y_val, knn.predict(X_val), digits=4))

"""**NN**"""

classifier = MLPClassifier(hidden_layer_sizes=(10,10,10), 
                           max_iter=500,
                           activation = 'relu',
                           solver='adam',
                           random_state=1)
classifier.fit(X_train, y_train)
print(classification_report(y_val, classifier.predict(X_val), digits=4))
